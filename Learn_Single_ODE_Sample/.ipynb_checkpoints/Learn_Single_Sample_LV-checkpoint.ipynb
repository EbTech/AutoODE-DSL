{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neural_odes import *\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_odes import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the Discrete Lotka-Volterra Eqtn for general non-symmetric $A$\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    p_i(t+1) &= p_i(t)\\big[1+r_i\\big(1-\\frac{\\sum_{j=1}^dA_{ij}p_j(t)}{k_i}\\big)\\big], i = 1, \\dots d\\\\\n",
    "    &= p_i(t)\\big[1+r_i\\big(1-\\frac{\\mathbf{b}_i^T\\big(\\sum_{j=1}^d\\mathbf{c}_jp_j(t)\\big)}{k_i}\\big)\\big], i = 1, \\dots d \\\\\n",
    "    &= p_i(t)\\big[1+r_i\\big(1-\\frac{\\mathbf{b}_i^TC\\mathbf{p}}{k_i}\\big)\\big], i = 1, \\dots d \\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate $A_{ij} = \\mathbf{b}_i^T\\mathbf{c}_j$ using the low rank matrix approximation $A= B^TC$, where $B = [\\mathbf{b}_1, \\cdots, \\mathbf{b}_d] \\in \\mathbb{R}^{k \\times d}$ and $C = [\\mathbf{c}_1, \\cdots, \\mathbf{c}_d] \\in \\mathbb{R}^{k \\times d}$.  Each $\\mathbf{b}_i, \\mathbf{c}_i \\in \\mathbb{R}^k$, where $k \\ll d$ are the embeddings of time series $i$.\n",
    "\n",
    "In matrix-vector form, we have $A\\mathbf{p}$, which has computational complexity $\\mathcal{O}(d^2)$ for $A \\in \\mathbb{R}^{d \\times d}, \\mathbf{p} \\in \\mathbb{R}^d$.  Using the low-rank form, we can write $A\\mathbf{p} = B^T(C \\mathbf{p})$.  We do not want to explicitly form the matrix $B^TC$, since this would have higher complexity of $\\mathcal{O}(kd^2)$.  We instead break the computation into two matrix-vector products as indicated by the parathesis, each of complexity $\\mathcal{O}(kd) \\ll \\mathcal{O}(d^2).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with $d = 100$ for the number of time series and will learn the synthetic data from the equation for random initialized $A, \\mathbf{r}, \\mathbf{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the LV eqn for $p_i(t+1), 0 \\le t < N - 1$.  We store $P$ as a matrix in $\\mathbb{R}^{d \\times N}$, whose first column is the initial condition $\\mathbf{p}(0) \\in \\mathbb{R}^d$. Then $P = [\\mathbf{p}(0), \\dots, \\mathbf{p}(N-1)].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Equation Test Case: symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps = 2\n",
    "num_time_series = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = torch.cuda.FloatTensor([1,3,0])\n",
    "k = torch.cuda.FloatTensor([2,3,1])\n",
    "B = torch.cuda.FloatTensor([[1, 0, 1], [2, 1, 1]])\n",
    "r = torch.cuda.FloatTensor([1,2,4])\n",
    "A = torch.mm(B.transpose(1,0), B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sym = True\n",
    "is_full_matrix = True\n",
    "low_rank_param = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 1.],\n",
       "         [2., 1., 1.]], device='cuda:0'), tensor([[5., 2., 3.],\n",
       "         [2., 1., 1.],\n",
       "         [3., 1., 2.]], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-3.5, dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test formula matches for i = 0\n",
    "((1 + r[0] * (1 - (A[0,0]*p0[0] + A[0,1] * p0[1] + A[0,2] * p0[2]) / k[0])) * p0[0]).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-0.99999976, dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test formula matches for i = 1\n",
    "((1 + r[1] * (1 - (A[1,0]*p0[0] + A[1,1] * p0[1] + A[1,2] * p0[2]) / k[1])) * p0[1]).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-0., dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test formula matches for i = 2\n",
    "((1 + r[2] * (1 - (A[2,0]*p0[0] + A[2,1] * p0[1] + A[2,2] * p0[2]) / k[2])) * p0[2]).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_lv = NeuralLV(num_time_series, num_time_steps, low_rank_param, is_full_matrix, p0, r, k, A, is_sym)\n",
    "p = neural_lv.solve_discrete_lv(A)\n",
    "p.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -3.5       ],\n",
       "       [ 3.        , -0.99999976],\n",
       "       [ 0.        , -0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_low_rank_sym = neural_lv.solve_discrete_lv(B, is_full_matrix=False)\n",
    "p_low_rank_sym.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -3.5       ],\n",
       "       [ 3.        , -0.99999976],\n",
       "       [ 0.        , -0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = None # symmetric case\n",
    "# Check case where explicitly forming A too\n",
    "A = compute_low_rank_product(B, C)\n",
    "p_low_rank_prod_sym = neural_lv.solve_discrete_lv(A, is_full_matrix=True)\n",
    "p_low_rank_prod_sym.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Equation Test Case: nonsymmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_lv = NeuralLV(num_time_series, num_time_steps, low_rank_param, is_full_matrix, p0, r, k, A, is_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.cuda.FloatTensor([[0, 2, 3], [-1, -2, 0]])\n",
    "A = torch.mm(B.transpose(1,0), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  6.],\n",
       "       [ 3., 23.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = neural_lv.solve_discrete_lv(A)\n",
    "p.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  6.],\n",
       "       [ 3., 23.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_low_rank_nonsym = neural_lv.solve_discrete_lv(B, C, is_full_matrix=False)\n",
    "p_low_rank_nonsym.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  6.],\n",
       "       [ 3., 23.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check case where explicitly forming A too\n",
    "A = compute_low_rank_product(B, C)\n",
    "p_low_rank_prod_sym = neural_lv.solve_discrete_lv(A, is_full_matrix=True)\n",
    "p_low_rank_prod_sym.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D LV Example of chaotic competitive LV systems\n",
    "In this section, we verify that the equation solver works on this common LV test case (See https://en.wikipedia.org/wiki/Competitive_Lotkaâ€“Volterra_equations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of time series = 4\n",
      "The number of time steps = 50\n"
     ]
    }
   ],
   "source": [
    "num_time_series = 4\n",
    "num_time_steps = 50\n",
    "low_rank_param = 5\n",
    "is_sym = False\n",
    "is_full_matrix = False\n",
    "print(f'The number of time series = {num_time_series}')\n",
    "print(f'The number of time steps = {num_time_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), torch.Size([4]), torch.Size([4]), torch.Size([4, 4]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)\n",
    "_, _, p0, A = generate_data(num_time_series)\n",
    "# r = torch.randn(num_time_series).float().to(device)\n",
    "k = torch.ones(num_time_series).float().to(device)\n",
    "\n",
    "r = torch.cuda.FloatTensor([1.0, 0.72, 1.53, 1.27])\n",
    "A = torch.cuda.FloatTensor([[1.0, 1.09, 1.52, 0.0], [0.0, 1.0, 0.44, 1.36], \n",
    "                             [2.33, 0.0, 1.0, 0.47], [1.21, 0.51, 0.35, 1.0]])\n",
    "# k = torch.abs(torch.randn(num_time_series).float().cuda())\n",
    "p0 = torch.ones(num_time_series).float().cuda()/10\n",
    "r.shape, k.shape, p0.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use embedding to learn $\\mathbf{b}_i \\in \\mathbb{R}^k$\n",
    "\n",
    "We are testing low rank symmetric case even though $A$ is not symmetric.  We will see in another notebook how computing two low rank matrices for non-symmetric $A$ improves the learning and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankVectorEmbedding(nn.Module):\n",
    "    def __init__(self, neural_lv):\n",
    "        super(LowRankVectorEmbedding, self).__init__()\n",
    "        self.num_time_series = neural_lv.num_time_series # num_ts\n",
    "        self.low_rank_param = neural_lv.low_rank_param # low rank parameter k\n",
    "        self.is_full_matrix = neural_lv.is_full_matrix\n",
    "        self.feat_static_cat = torch.arange(self.num_time_series).to(device)\n",
    "        self.is_sym = neural_lv.is_sym\n",
    "        self.neural_lv = neural_lv        \n",
    "      \n",
    "        self.embedding_low_rank_mat_B = nn.Embedding(self.num_time_series, self.low_rank_param)\n",
    "        self.embedding_low_rank_mat_B.weight.data.uniform_(-0.1, 0.1)\n",
    "        if not self.is_sym:\n",
    "            self.embedding_low_rank_mat_C = nn.Embedding(self.num_time_series, self.low_rank_param)\n",
    "            self.embedding_low_rank_mat_C.weight.data.uniform_(-0.1, 0.1)\n",
    "    def forward(self):\n",
    "        # find low rank vector computed per time series\n",
    "        # feat_static_cat consists of the time series indices 0, ..., cardinality - 1\n",
    "        # embedding returns (num_ts, low_rank_param) need to transpose it\n",
    "        B = self.embedding_low_rank_mat_B(self.feat_static_cat).T\n",
    "        C = None if self.is_sym else self.embedding_low_rank_mat_C(self.feat_static_cat).T\n",
    "        # Explicitly form matrix matrix product A = B^T* CO(kd^2) expensive\n",
    "        if self.is_full_matrix:\n",
    "            return self.neural_lv.solve_discrete_lv(compute_low_rank_product(B, C))\n",
    "        # Compute matrix vector product B^T * (C p) O(kd)\n",
    "        else:\n",
    "            return self.neural_lv.solve_discrete_lv(B, C, self.is_full_matrix)\n",
    "        \n",
    "        \n",
    "class NeuralLV(nn.Module):\n",
    "    def __init__(self, num_time_series, num_time_steps, low_rank_param, \n",
    "                 is_full_matrix, p0, r, k, A, is_sym):\n",
    "        super(NeuralLV, self).__init__()\n",
    "        # Define number of time series\n",
    "        self.num_time_series = num_time_series\n",
    "        # Define number of discrete time steps will assume the same for each time series so p_i(t) in R^(dxN)\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.low_rank_param = low_rank_param\n",
    "        self.is_full_matrix = is_full_matrix\n",
    "        self.p0 = p0\n",
    "        self.true_r = r\n",
    "        self.r = nn.Parameter(torch.rand(num_time_series).float().cuda())#r\n",
    "        self.true_k = k\n",
    "        self.k = nn.Parameter(torch.rand(num_time_series).float().cuda())\n",
    "        self.A = A\n",
    "        self.is_sym = is_sym\n",
    "        \n",
    "        \n",
    "        self.feat_static_cat = torch.arange(self.num_time_series).to(device)\n",
    "        \n",
    "        #self.neural_lv = neural_lv        \n",
    "      \n",
    "        self.embedding_low_rank_mat_B = nn.Embedding(self.num_time_series, self.low_rank_param)\n",
    "        self.embedding_low_rank_mat_B.weight.data.uniform_(-0.1, 0.1)\n",
    "        if not self.is_sym:\n",
    "            self.embedding_low_rank_mat_C = nn.Embedding(self.num_time_series, self.low_rank_param)\n",
    "            self.embedding_low_rank_mat_C.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    \n",
    "    def solve_discrete_lv(self, mat1, mat2=None, is_full_matrix=True, is_target = False):\n",
    "        p = [] # need to store as list for autograd won't let you append indices in same matrix\n",
    "        p.append(self.p0)\n",
    "        for n in range(self.num_time_steps-1): # element-wise vector division and multiplication\n",
    "            # Compute Ap to generate synthetic data for the full rank matrix A\n",
    "            if is_full_matrix:\n",
    "                #print(mat1.shape, p[n].shape)\n",
    "                mat_vec_prod = torch.mm(mat1, p[n].reshape(-1, 1)).squeeze(-1)\n",
    "            else:\n",
    "                mat_vec_prod = compute_mat_vec_prod(mat1, mat2, p[n].reshape(-1, 1)).squeeze(-1)\n",
    "            if is_target:\n",
    "                p.append((1 + self.true_r * (1 - mat_vec_prod / self.true_k)) * p[n])\n",
    "            else:\n",
    "                p.append((1 + self.r * (1 - mat_vec_prod)) * p[n])#/ self.k\n",
    "\n",
    "        return torch.cat(p, dim=0).reshape(self.num_time_steps, self.num_time_series).T\n",
    "    \n",
    "    def forward(self):\n",
    "        B = self.embedding_low_rank_mat_B(self.feat_static_cat).T\n",
    "        C = None if self.is_sym else self.embedding_low_rank_mat_C(self.feat_static_cat).T\n",
    "        \n",
    "        if self.is_full_matrix:\n",
    "            p_approx = self.solve_discrete_lv(compute_low_rank_product(B, C))\n",
    "        else:\n",
    "            p_approx = self.solve_discrete_lv(B, C, self.is_full_matrix)\n",
    "        p = self.solve_discrete_lv(self.A, is_target = True)\n",
    "        return p, p_approx, compute_low_rank_product(B, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_series = 4\n",
    "num_time_steps = 10\n",
    "is_sym = False\n",
    "is_full_matrix = True\n",
    "low_rank_param = 1\n",
    "\n",
    "model = NeuralLV(num_time_series, num_time_steps, low_rank_param, is_full_matrix, p0, r, k, A, is_sym).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma= 0.9)\n",
    "loss_fun = torch.nn.MSELoss()#SmoothL1Loss()#\n",
    "tqdm_epochs = tqdm(range(30))\n",
    "for e in tqdm_epochs:\n",
    "    p, p_approx, A_approx = model.forward()\n",
    "    Loss = loss_fun(p, p_approx)\n",
    "    optimizer.zero_grad()\n",
    "    Loss.backward()#retain_graph=True\n",
    "    optimizer.step()\n",
    "    tqdm_epochs.set_postfix({'loss': Loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_plots=10\n",
    "num_rows=2\n",
    "fig_size_width=10\n",
    "plt.rcParams[\"figure.figsize\"] = (fig_size_width, 5)\n",
    "num_ts = p.shape[0]\n",
    "N = p.shape[1]\n",
    "t = np.arange(N)\n",
    "num_plots = min(num_ts, max_num_plots)\n",
    "num_cols = int(num_plots / num_rows)\n",
    "fig, axs = plt.subplots(num_rows, num_cols)\n",
    "for ts_idx in range(num_plots):\n",
    "    plt.subplot(num_rows, num_cols, ts_idx+1) \n",
    "    plt.plot(t, p[ts_idx, :].cpu().data.numpy(), \\\n",
    "             t, p_approx[ts_idx, :].cpu().data.numpy(), 'r--')\n",
    "    plt.ylabel(f'$p_{ts_idx}(t)$')\n",
    "    plt.xlabel('t')\n",
    "    plt.legend(('Exact', 'Approx'))\n",
    "    plt.xlabel('time: $t$')\n",
    "    plt.ylabel(f'$p_{ts_idx}(t)$')\n",
    "#plt.savefig(\"learn_k_r_10_nonsym.png\", dpi = 300 , bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Errors and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 norm of the error = 0.011699080467224121\n"
     ]
    }
   ],
   "source": [
    "print(f'l2 norm of the error = {torch.sqrt(torch.mean((p_approx-p)**2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max norm of the error = 0.03163820505142212\n"
     ]
    }
   ],
   "source": [
    "print(f'max norm of the error = {torch.max(torch.abs(p_approx-p))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 matrix norm of the error of A and its low rank approx = 0.6239166259765625\n"
     ]
    }
   ],
   "source": [
    "print(f'l2 matrix norm of the error of A and its low rank approx = {torch.sqrt(torch.mean((A_approx-A)**2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.8065, 0.9372, 1.6270, 1.6514], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.r, model.A, A_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_plot_ts(p, p_approx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
